{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOz4MMDYvdEQ51pxWhHW0Mz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a53e8eb0cf51414fa7a9ad4abbd65e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c4df821ab14d4144bc31559aebf95e2b",
              "IPY_MODEL_ec04bcc252b541919343917bc9907009",
              "IPY_MODEL_35381c867ed5474db2418e031396601e"
            ],
            "layout": "IPY_MODEL_4dc1878967d044f48aaa6a3db3e73ba7"
          }
        },
        "c4df821ab14d4144bc31559aebf95e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28512038a4d449edabdff3ed686faea5",
            "placeholder": "​",
            "style": "IPY_MODEL_214ccb8db89246c7aa0118d4c6cdc0df",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "ec04bcc252b541919343917bc9907009": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a63a4cdd07234841b6fba3a4e9d84283",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0347bca12f84efb890ad960948cf5c4",
            "value": 48
          }
        },
        "35381c867ed5474db2418e031396601e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fcd098c7e0944ed8b7ecead526e99dd",
            "placeholder": "​",
            "style": "IPY_MODEL_042a23864a9e44cab18e617ee2597f39",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.69kB/s]"
          }
        },
        "4dc1878967d044f48aaa6a3db3e73ba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28512038a4d449edabdff3ed686faea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "214ccb8db89246c7aa0118d4c6cdc0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a63a4cdd07234841b6fba3a4e9d84283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0347bca12f84efb890ad960948cf5c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8fcd098c7e0944ed8b7ecead526e99dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "042a23864a9e44cab18e617ee2597f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9b6e89b6e1d04019a19420dc1b7d3d81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a9c734c579814360a1b0b74e994ef581",
              "IPY_MODEL_8d5daf54334d4a72838fbbdd764dd355",
              "IPY_MODEL_bfa5ffc7d7bb4dc79b9e376a367a9261"
            ],
            "layout": "IPY_MODEL_a421cf45221d41b8855650a748bd68e4"
          }
        },
        "a9c734c579814360a1b0b74e994ef581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8a8cd80596147bbaf995673251f1aa4",
            "placeholder": "​",
            "style": "IPY_MODEL_9733d86304184e0b836721cb93e6f259",
            "value": "config.json: 100%"
          }
        },
        "8d5daf54334d4a72838fbbdd764dd355": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95c0e0d8ead34ba697c1b3be8ed573f0",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d2c85ada5c9403aae8441f453c082fb",
            "value": 483
          }
        },
        "bfa5ffc7d7bb4dc79b9e376a367a9261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_934a0ad0f0a347a290914e04c7a83df1",
            "placeholder": "​",
            "style": "IPY_MODEL_0c220dfcceeb49e3a4b481c1d369d304",
            "value": " 483/483 [00:00&lt;00:00, 21.5kB/s]"
          }
        },
        "a421cf45221d41b8855650a748bd68e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8a8cd80596147bbaf995673251f1aa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9733d86304184e0b836721cb93e6f259": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95c0e0d8ead34ba697c1b3be8ed573f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d2c85ada5c9403aae8441f453c082fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "934a0ad0f0a347a290914e04c7a83df1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c220dfcceeb49e3a4b481c1d369d304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1986a563b6fb46f78f96f018164ab22b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02b09114801b49d6bde413cf9ea4a417",
              "IPY_MODEL_de8bcc1ddab74a7b86ef104c3f752b55",
              "IPY_MODEL_4651a6a505b44822b180d0c3c3a708d6"
            ],
            "layout": "IPY_MODEL_465a694a7f8546c88eb673df1ffac171"
          }
        },
        "02b09114801b49d6bde413cf9ea4a417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fe01b050981435aaa69a9f4b395e23d",
            "placeholder": "​",
            "style": "IPY_MODEL_84e2aeb72d36413cb1fbcf3634446a77",
            "value": "vocab.txt: 100%"
          }
        },
        "de8bcc1ddab74a7b86ef104c3f752b55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_add85205bd1a44a3981696db172c89cf",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92d3c282457d40588a742f6fffac2952",
            "value": 231508
          }
        },
        "4651a6a505b44822b180d0c3c3a708d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74eaed45025640edaf19ce3dfbccc4cb",
            "placeholder": "​",
            "style": "IPY_MODEL_975972af84a649bfa82de33036770525",
            "value": " 232k/232k [00:00&lt;00:00, 2.94MB/s]"
          }
        },
        "465a694a7f8546c88eb673df1ffac171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fe01b050981435aaa69a9f4b395e23d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84e2aeb72d36413cb1fbcf3634446a77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "add85205bd1a44a3981696db172c89cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92d3c282457d40588a742f6fffac2952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74eaed45025640edaf19ce3dfbccc4cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "975972af84a649bfa82de33036770525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdd5a1487c8940148601c07a5ab17502": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ec29b165364f4b38a4d6b062e81adda0",
              "IPY_MODEL_32ab07f9346c401ba9ce623a3345cd20",
              "IPY_MODEL_d1938e1acc6c4ba1a133ba4f1084f7df"
            ],
            "layout": "IPY_MODEL_9503b10dff414959a144cf4ecea37518"
          }
        },
        "ec29b165364f4b38a4d6b062e81adda0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82dc2f7cd6564f50a67c9597377bfe0f",
            "placeholder": "​",
            "style": "IPY_MODEL_b9b80ffc4cde4323b7cf5da2e729bb39",
            "value": "tokenizer.json: 100%"
          }
        },
        "32ab07f9346c401ba9ce623a3345cd20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48e476d91c034a71b1534ae46fdeabd4",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc673d0ab9ba41af89171f2d0807e580",
            "value": 466062
          }
        },
        "d1938e1acc6c4ba1a133ba4f1084f7df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b27385a37b524df5b6a1db832ed1e0a4",
            "placeholder": "​",
            "style": "IPY_MODEL_523a9ba7cdeb4ca0af485fdbbb6d0495",
            "value": " 466k/466k [00:00&lt;00:00, 6.70MB/s]"
          }
        },
        "9503b10dff414959a144cf4ecea37518": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82dc2f7cd6564f50a67c9597377bfe0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9b80ffc4cde4323b7cf5da2e729bb39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48e476d91c034a71b1534ae46fdeabd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc673d0ab9ba41af89171f2d0807e580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b27385a37b524df5b6a1db832ed1e0a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "523a9ba7cdeb4ca0af485fdbbb6d0495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f10fba74bfd64d919923c4bd0581d495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5785c973cc464f309938a72d8742cf76",
              "IPY_MODEL_7139b403b4a2426fb07489859019cebc",
              "IPY_MODEL_f7772f603f3b4648b963a625e5b66542"
            ],
            "layout": "IPY_MODEL_770b6f17358b441eb12cf88e3ed47604"
          }
        },
        "5785c973cc464f309938a72d8742cf76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1dea71beaaa4c929808863414363e52",
            "placeholder": "​",
            "style": "IPY_MODEL_b95a15bd195047e8a73dc7f44e6f53b2",
            "value": "model.safetensors: 100%"
          }
        },
        "7139b403b4a2426fb07489859019cebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_992695db437040de94b906f8aa8228bc",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_26211c69f49f4a75a6915d844ad97ef9",
            "value": 267954768
          }
        },
        "f7772f603f3b4648b963a625e5b66542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f8feac08a494b28a7cbc0a51c86b4d1",
            "placeholder": "​",
            "style": "IPY_MODEL_52192371a3144c5d8ff768889d4851f4",
            "value": " 268M/268M [00:03&lt;00:00, 108MB/s]"
          }
        },
        "770b6f17358b441eb12cf88e3ed47604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1dea71beaaa4c929808863414363e52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b95a15bd195047e8a73dc7f44e6f53b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "992695db437040de94b906f8aa8228bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26211c69f49f4a75a6915d844ad97ef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f8feac08a494b28a7cbc0a51c86b4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52192371a3144c5d8ff768889d4851f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84098f5f42fb418ba1f879bc713fa960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87624f56c0424f43b1d28b7dcdc9697e",
              "IPY_MODEL_96a0989d1e594b43a373757e44771049",
              "IPY_MODEL_1d6b8def3ece4b9e972265b73f7813ad"
            ],
            "layout": "IPY_MODEL_d86c22c037ed4c69af1373da32029e18"
          }
        },
        "87624f56c0424f43b1d28b7dcdc9697e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_816a405cdb68465fbba88e66c26ec57f",
            "placeholder": "​",
            "style": "IPY_MODEL_4fe9882cd1d54865a7192b988af1c5a7",
            "value": "Map: 100%"
          }
        },
        "96a0989d1e594b43a373757e44771049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d21606ab9904151a96dcc5ed6256ed3",
            "max": 181,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1eedc92933dd45ebb8cbd2cd1cff643f",
            "value": 181
          }
        },
        "1d6b8def3ece4b9e972265b73f7813ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_227781ce1b1f4b78801390c1e35af64a",
            "placeholder": "​",
            "style": "IPY_MODEL_b9fced6686b2495690cb867c005ed3c0",
            "value": " 181/181 [00:00&lt;00:00, 732.36 examples/s]"
          }
        },
        "d86c22c037ed4c69af1373da32029e18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "816a405cdb68465fbba88e66c26ec57f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fe9882cd1d54865a7192b988af1c5a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d21606ab9904151a96dcc5ed6256ed3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eedc92933dd45ebb8cbd2cd1cff643f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "227781ce1b1f4b78801390c1e35af64a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9fced6686b2495690cb867c005ed3c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badri-shivani/daa_gpt/blob/main/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTDJ2kJEIAfz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5dfa527"
      },
      "source": [
        "# Task\n",
        "Create a tutor chatbot by uploading files and training a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0ec38e9"
      },
      "source": [
        "## Understand the requirements\n",
        "\n",
        "### Subtask:\n",
        "Clarify the type of tutoring the chatbot should provide and the format of the uploaded files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83703ded"
      },
      "source": [
        "**Reasoning**:\n",
        "Engage with the user to clarify the type of tutoring, the level, and the file formats they plan to upload to create the tutor chatbot.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f980421e",
        "outputId": "6c35b75f-31dc-4e8b-a5ad-aba364d0ab08"
      },
      "source": [
        "print(\"Hello! I can help you create a tutor chatbot. To get started, please tell me:\")\n",
        "print(\"1. What specific subject or area should the chatbot tutor in?\")\n",
        "print(\"2. What is the expected level of tutoring (e.g., beginner, intermediate, advanced)?\")\n",
        "print(\"3. What types of files do you plan to upload (e.g., PDF documents, text files, code files, specific data formats like CSV or JSON)?\")\n",
        "print(\"4. Are there any specific formatting or structure requirements for the content within these files that the chatbot should be aware of (e.g., specific headings, sections, or data structures)?\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I can help you create a tutor chatbot. To get started, please tell me:\n",
            "1. What specific subject or area should the chatbot tutor in?\n",
            "2. What is the expected level of tutoring (e.g., beginner, intermediate, advanced)?\n",
            "3. What types of files do you plan to upload (e.g., PDF documents, text files, code files, specific data formats like CSV or JSON)?\n",
            "4. Are there any specific formatting or structure requirements for the content within these files that the chatbot should be aware of (e.g., specific headings, sections, or data structures)?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2aa935d"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "### Subtask:\n",
        "Load and preprocess the uploaded files to extract the relevant information for the chatbot. This might involve techniques like text extraction, cleaning, and structuring the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aca896c"
      },
      "source": [
        "## Data preparation\n",
        "\n",
        "### Subtask:\n",
        "Load and preprocess the uploaded files to extract the relevant information for the chatbot. This might involve techniques like text extraction, cleaning, and structuring the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2efdcaa"
      },
      "source": [
        "**Reasoning**:\n",
        "Request the user to specify the file types they will be uploading and explain why this information is needed to proceed with data loading and preprocessing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84b7c2ea",
        "outputId": "beba56c0-05f4-48e3-b7dc-a83d6550eaab"
      },
      "source": [
        "print(\"To load and preprocess the uploaded files for the chatbot, I need to know the types of files you will be providing.\")\n",
        "print(\"This information was not specified previously and is crucial for determining the appropriate method for extracting and cleaning the data.\")\n",
        "print(\"Please let me know the file types (e.g., PDF, plain text, etc.).\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To load and preprocess the uploaded files for the chatbot, I need to know the types of files you will be providing.\n",
            "This information was not specified previously and is crucial for determining the appropriate method for extracting and cleaning the data.\n",
            "Please let me know the file types (e.g., PDF, plain text, etc.).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd420f8c"
      },
      "source": [
        "## Choose a model\n",
        "\n",
        "### Subtask:\n",
        "Choose a suitable model architecture for the chatbot.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8aa9cf0"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the task requirements for a tutor chatbot and the typical capabilities needed for conversational AI and question answering from potentially unstructured text data, a transformer-based architecture, specifically a pre-trained language model fine-tuned for question answering, is a suitable choice. This approach leverages the extensive knowledge embedded in large pre-trained models and can be adapted to the specific tutoring domain with relatively less data than training a model from scratch. Given that the file types are not yet known for sure but are expected to contain text-based educational content, a model adept at processing and understanding natural language is necessary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05b210ea",
        "outputId": "3f6e2ced-7514-4c87-8013-15983f482557"
      },
      "source": [
        "print(\"For a tutor chatbot, a transformer-based architecture, specifically a pre-trained language model fine-tuned for question answering, is a highly suitable choice.\")\n",
        "print(\"These models, such as those based on the BERT, GPT, or T5 architectures, have demonstrated strong capabilities in understanding context and generating relevant responses, which are essential for a conversational tutoring system.\")\n",
        "print(\"Using a pre-trained model allows us to leverage the vast amount of knowledge they have learned from diverse text data, reducing the need for a massive domain-specific dataset for initial training.\")\n",
        "print(\"The pre-trained model can then be fine-tuned on the specific tutoring content provided in the uploaded files, allowing it to adapt to the subject matter and answer questions accurately within that domain.\")\n",
        "print(\"This approach balances the need for strong language understanding with potentially limited domain-specific training data, making it efficient and effective for building a tutor chatbot.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For a tutor chatbot, a transformer-based architecture, specifically a pre-trained language model fine-tuned for question answering, is a highly suitable choice.\n",
            "These models, such as those based on the BERT, GPT, or T5 architectures, have demonstrated strong capabilities in understanding context and generating relevant responses, which are essential for a conversational tutoring system.\n",
            "Using a pre-trained model allows us to leverage the vast amount of knowledge they have learned from diverse text data, reducing the need for a massive domain-specific dataset for initial training.\n",
            "The pre-trained model can then be fine-tuned on the specific tutoring content provided in the uploaded files, allowing it to adapt to the subject matter and answer questions accurately within that domain.\n",
            "This approach balances the need for strong language understanding with potentially limited domain-specific training data, making it efficient and effective for building a tutor chatbot.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "027a3fec"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train the chosen model using the prepared data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b335c03"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train the chosen model using the prepared data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cde31ece"
      },
      "source": [
        "**Reasoning**:\n",
        "Acknowledge that data preparation is blocked and explain that model training cannot proceed without prepared data, and briefly reiterate the next steps once data is ready.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3629a6dd",
        "outputId": "d0180a56-06b0-4a1d-8458-13cad045a822"
      },
      "source": [
        "print(\"Acknowledgement: The data preparation step is currently blocked because the file types for the uploaded data have not been provided.\")\n",
        "print(\"As a result, the model training process cannot proceed at this time, as it is dependent on having the prepared data available.\")\n",
        "print(\"Once the file types are provided and the data preparation is complete, the next steps for model training will involve:\")\n",
        "print(\"1. Loading a pre-trained model suitable for the tutoring task.\")\n",
        "print(\"2. Preparing the data in the required format for fine-tuning the chosen model.\")\n",
        "print(\"3. Configuring the training parameters (e.g., learning rate, epochs, batch size).\")\n",
        "print(\"4. Executing the training process to fine-tune the model on the prepared data.\")\n",
        "print(\"These steps will be provided once the data is ready.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acknowledgement: The data preparation step is currently blocked because the file types for the uploaded data have not been provided.\n",
            "As a result, the model training process cannot proceed at this time, as it is dependent on having the prepared data available.\n",
            "Once the file types are provided and the data preparation is complete, the next steps for model training will involve:\n",
            "1. Loading a pre-trained model suitable for the tutoring task.\n",
            "2. Preparing the data in the required format for fine-tuning the chosen model.\n",
            "3. Configuring the training parameters (e.g., learning rate, epochs, batch size).\n",
            "4. Executing the training process to fine-tune the model on the prepared data.\n",
            "These steps will be provided once the data is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61a1f03d"
      },
      "source": [
        "## Model evaluation\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model's performance to ensure it meets the requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bb879b8"
      },
      "source": [
        "**Reasoning**:\n",
        "Acknowledge the dependency on data preparation and model training for evaluation, state that metrics will be determined and executed after successful training, and explain the likely nature of the evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39ef4f4d",
        "outputId": "bcce89fb-d11b-44cb-c880-b09598e62711"
      },
      "source": [
        "print(\"Acknowledgement: The evaluation of the trained model's performance is contingent on the successful completion of both the data preparation and model training steps.\")\n",
        "print(\"Since these steps are currently pending the provision of file types for the uploaded data, model evaluation cannot proceed at this time.\")\n",
        "print(\"Once the data has been prepared and the model successfully trained, the specific evaluation metrics and procedures will be determined and executed.\")\n",
        "print(\"Evaluation will likely involve assessing the model's ability to provide accurate, relevant, and helpful answers to questions based on the uploaded tutoring content.\")\n",
        "print(\"This will help ensure the chatbot meets the requirements for effective tutoring.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acknowledgement: The evaluation of the trained model's performance is contingent on the successful completion of both the data preparation and model training steps.\n",
            "Since these steps are currently pending the provision of file types for the uploaded data, model evaluation cannot proceed at this time.\n",
            "Once the data has been prepared and the model successfully trained, the specific evaluation metrics and procedures will be determined and executed.\n",
            "Evaluation will likely involve assessing the model's ability to provide accurate, relevant, and helpful answers to questions based on the uploaded tutoring content.\n",
            "This will help ensure the chatbot meets the requirements for effective tutoring.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcd5346e"
      },
      "source": [
        "## Deployment\n",
        "\n",
        "### Subtask:\n",
        "Deploy the trained chatbot model so it can be used to interact with users.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e29bf19"
      },
      "source": [
        "**Reasoning**:\n",
        "Acknowledge that deployment is dependent on successful data preparation, model training, and model evaluation. Explain the deployment strategy and mention that the chosen method will involve making the trained model accessible for real-time interaction. State that the specific technical steps for deployment will be provided once the model is ready.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26732bc7",
        "outputId": "483da13b-2e5a-4bd7-9353-f814ea0d4992"
      },
      "source": [
        "print(\"Acknowledgement: The deployment of the tutor chatbot model is dependent on the successful completion of the data preparation, model training, and model evaluation phases.\")\n",
        "print(\"Currently, these preceding steps are pending, as the file types for the uploaded data are still required to initiate the data preparation process.\")\n",
        "print(\"Once these foundational steps are successfully completed and the model is ready for deployment, the specific deployment strategy will be determined.\")\n",
        "print(\"The choice of deployment method will be based on the requirements for how users will interact with the chatbot (e.g., through a web application, an API endpoint for integration into another platform, or a standalone application).\")\n",
        "print(\"Regardless of the specific method chosen, the deployment process will involve making the trained model accessible and functional for real-time interaction with users, allowing them to ask questions and receive tutoring.\")\n",
        "print(\"The detailed technical steps for deploying the model will be provided once the data is prepared, the model is trained, and its performance has been evaluated and deemed satisfactory.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acknowledgement: The deployment of the tutor chatbot model is dependent on the successful completion of the data preparation, model training, and model evaluation phases.\n",
            "Currently, these preceding steps are pending, as the file types for the uploaded data are still required to initiate the data preparation process.\n",
            "Once these foundational steps are successfully completed and the model is ready for deployment, the specific deployment strategy will be determined.\n",
            "The choice of deployment method will be based on the requirements for how users will interact with the chatbot (e.g., through a web application, an API endpoint for integration into another platform, or a standalone application).\n",
            "Regardless of the specific method chosen, the deployment process will involve making the trained model accessible and functional for real-time interaction with users, allowing them to ask questions and receive tutoring.\n",
            "The detailed technical steps for deploying the model will be provided once the data is prepared, the model is trained, and its performance has been evaluated and deemed satisfactory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "86dea6fa",
        "outputId": "840b48d3-7db7-42ad-e95b-7064ddae650a"
      },
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b89c2f57-7c18-4b45-8ec7-fb33c1914ab3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b89c2f57-7c18-4b45-8ec7-fb33c1914ab3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving daa_gpt-main.zip to daa_gpt-main.zip\n",
            "User uploaded file \"daa_gpt-main.zip\" with length 2646359 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: my code is written down please i want to train the model addthe code fortraining the model\n",
        "\n",
        "!pip install transformers datasets\n",
        "!pip install accelerate -U\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "# Placeholder: Load and preprocess your data here\n",
        "# This is a crucial step that depends on the file types you uploaded.\n",
        "# You need to parse the text from your files and structure it in a way\n",
        "# suitable for Question Answering (e.g., SQuAD format).\n",
        "\n",
        "# For demonstration purposes, let's assume you have your data structured\n",
        "# into a list of dictionaries, where each dictionary represents a context\n",
        "# and associated question-answer pairs.\n",
        "# Example structure (you'll need to replace this with your actual loaded data):\n",
        "# data = [\n",
        "#     {\n",
        "#         'context': 'Your context text from your document.',\n",
        "#         'qas': [\n",
        "#             {\n",
        "#                 'id': 'unique_qa_id_1',\n",
        "#                 'question': 'A question about the context?',\n",
        "#                 'answers': [{'answer_start': 10, 'text': 'The answer'}]\n",
        "#             }\n",
        "#         ]\n",
        "#     },\n",
        "#     # Add more contexts and Q&A pairs\n",
        "# ]\n",
        "\n",
        "# Assuming you have loaded and processed your data into a list named 'processed_data'\n",
        "# For this example, we'll create a dummy dataset structure similar to SQuAD\n",
        "processed_data = [] # Replace with your actual processed data\n",
        "\n",
        "# Example of how your processed_data *might* look like if you manually\n",
        "# extracted contexts and Q&A pairs from your files:\n",
        "# processed_data = [\n",
        "#     {\n",
        "#         'context': 'Photosynthesis is a process used by plants, algae and cyanobacteria to convert light energy into chemical energy, through a process using sunlight, water and carbon dioxide.',\n",
        "#         'qas': [\n",
        "#             {\n",
        "#                 'id': 'dummy_qa_1',\n",
        "#                 'question': 'What is photosynthesis?',\n",
        "#                 'answers': [{'answer_start': 0, 'text': 'a process used by plants, algae and cyanobacteria to convert light energy into chemical energy'}]\n",
        "#             },\n",
        "#              {\n",
        "#                 'id': 'dummy_qa_2',\n",
        "#                 'question': 'What do plants use for photosynthesis?',\n",
        "#                 'answers': [{'answer_start': 151, 'text': 'sunlight, water and carbon dioxide'}]\n",
        "#             }\n",
        "#         ]\n",
        "#     },\n",
        "#     {\n",
        "#         'context': 'The capital of France is Paris. Paris is known for the Eiffel Tower and the Louvre Museum.',\n",
        "#         'qas': [\n",
        "#             {\n",
        "#                 'id': 'dummy_qa_3',\n",
        "#                 'question': 'What is the capital of France?',\n",
        "#                 'answers': [{'answer_start': 19, 'text': 'Paris'}]\n",
        "#             }\n",
        "#         ]\n",
        "#     }\n",
        "# ]\n",
        "\n",
        "# IMPORTANT: Replace the above dummy_data with the data loaded and processed\n",
        "# from your uploaded files. The structure needs to be similar to SQuAD.\n",
        "# You will need to write code to read your specific file types (PDF, text, etc.)\n",
        "# and extract contexts and potential question-answer pairs. This is the most\n",
        "# challenging and dependent part.\n",
        "\n",
        "# If your data is not in Q&A format, you might need a different model\n",
        "# (e.g., for text generation or summarization) and a different training approach.\n",
        "# Assuming you are going with a Q&A approach and have processed your data:\n",
        "\n",
        "# Convert your processed data into a Hugging Face Dataset object\n",
        "# You need to flatten the structure if you have multiple qas per context\n",
        "flattened_data = []\n",
        "for item in processed_data:\n",
        "    context = item['context']\n",
        "    for qa in item['qas']:\n",
        "        flattened_data.append({\n",
        "            'id': qa['id'],\n",
        "            'context': context,\n",
        "            'question': qa['question'],\n",
        "            'answers': qa['answers']\n",
        "        })\n",
        "\n",
        "if not flattened_data:\n",
        "    print(\"No processed data found. Please ensure your data preparation step correctly populates 'processed_data'. Model training cannot proceed.\")\n",
        "else:\n",
        "    dataset = Dataset.from_list(flattened_data)\n",
        "\n",
        "    # Choose a pre-trained model and tokenizer\n",
        "    model_name = \"distilbert-base-uncased\" # You can choose other models like \"bert-base-uncased\", \"roberta-base\", etc.\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "    # Preprocess the dataset\n",
        "    def preprocess_function(examples):\n",
        "        questions = [q.strip() for q in examples[\"question\"]]\n",
        "        inputs = tokenizer(\n",
        "            questions,\n",
        "            examples[\"context\"],\n",
        "            max_length=384,\n",
        "            truncation=\"only_second\",\n",
        "            return_offsets_mapping=True,\n",
        "            padding=\"max_length\",\n",
        "        )\n",
        "\n",
        "        offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "        answers = examples[\"answers\"]\n",
        "        start_positions = []\n",
        "        end_positions = []\n",
        "\n",
        "        for i, offset in enumerate(offset_mapping):\n",
        "            start_char = answers[i][0][\"answer_start\"]\n",
        "            end_char = start_char + len(answers[i][0][\"text\"])\n",
        "\n",
        "            sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "            # Find the start and end of the context\n",
        "            idx = 0\n",
        "            while sequence_ids[idx] != 1:\n",
        "                idx += 1\n",
        "            context_start = idx\n",
        "            while sequence_ids[idx] == 1:\n",
        "                idx += 1\n",
        "            context_end = idx - 1\n",
        "\n",
        "            # If the answer is not fully inside the context, label it (0, 0)\n",
        "            if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
        "                start_positions.append(0)\n",
        "                end_positions.append(0)\n",
        "            else:\n",
        "                # Otherwise it's the start and end token positions\n",
        "                idx = context_start\n",
        "                while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                    idx += 1\n",
        "                start_positions.append(idx - 1)\n",
        "\n",
        "                idx = context_end\n",
        "                while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                    idx -= 1\n",
        "                end_positions.append(idx + 1)\n",
        "\n",
        "        inputs[\"start_positions\"] = start_positions\n",
        "        inputs[\"end_positions\"] = end_positions\n",
        "        return inputs\n",
        "\n",
        "    tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        evaluation_strategy=\"epoch\", # You might want to split your data for evaluation\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        weight_decay=0.01,\n",
        "    )\n",
        "\n",
        "    # Create the Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        # eval_dataset=tokenized_eval_dataset, # Add an evaluation dataset if you have one\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"\\nStarting model training...\")\n",
        "    trainer.train()\n",
        "    print(\"Model training complete.\")\n",
        "\n",
        "    # You can save the trained model\n",
        "    model_save_path = \"./trained_qa_model\"\n",
        "    trainer.save_model(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    print(f\"Trained model saved to {model_save_path}\")\n",
        "\n",
        "    # You are now ready to move to the evaluation and deployment steps.\n",
        "```"
      ],
      "metadata": {
        "id": "W7Nk6lVRKEW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the model once using session state\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    return pipeline(\n",
        "        \"text-generation\",\n",
        "        model=\"EleutherAI/gpt-neo-1.3B\",\n",
        "        device=-1,  # Set to 0 if you have a GPU locally\n",
        "    )\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# Title and subtitle\n",
        "st.title(\"🤖 DAA & OS GPT - Free Tutor\")\n",
        "st.subheader(\"Ask anything about Design and Analysis of Algorithms or Operating Systems.\")\n",
        "\n",
        "# Input box\n",
        "user_input = st.text_area(\"💬 Ask a question:\", height=100)\n",
        "\n",
        "# Answer generation\n",
        "if st.button(\"Get Answer\"):\n",
        "    if user_input.strip() == \"\":\n",
        "        st.warning(\"Please enter a question.\")\n",
        "    else:\n",
        "        prompt = (\n",
        "            \"You are an expert tutor for B.Tech students in Design and Analysis of Algorithms and Operating Systems.\\n\"\n",
        "            f\"Q: {user_input}\\nA:\"\n",
        "        )\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = model(\n",
        "                prompt,\n",
        "                max_length=150,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.95,\n",
        "                top_k=50,\n",
        "                num_return_sequences=1\n",
        "            )[0][\"generated_text\"]\n",
        "\n",
        "            answer = response.split(\"A:\")[-1].strip()\n",
        "\n",
        "            if not answer:\n",
        "                st.error(\"Hmm... couldn't generate a helpful response. Try rephrasing your question.\")\n",
        "            else:\n",
        "                st.success(\"Answer:\")\n",
        "                st.write(answer)\n",
        "\n"
      ],
      "metadata": {
        "id": "QVd-QBHnKEd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15dbb792",
        "outputId": "7d63c67d-f453-464c-cc14-96ee3f7214a5"
      },
      "source": [
        "!pip install pymupdf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa558405",
        "outputId": "3c4ce14f-4971-4372-d35b-3a066831a7cb"
      },
      "source": [
        "import fitz # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "# Placeholder for where you will call the extraction function for your uploaded PDFs\n",
        "# You'll need to list the PDF files from your extracted zip folder and loop through them.\n",
        "\n",
        "# Example (assuming your PDFs are in the 'uploaded_data' folder and end with .pdf):\n",
        "pdf_texts = {}\n",
        "extracted_folder = \"uploaded_data\" # Make sure this matches the folder name in the unzip step\n",
        "\n",
        "import os\n",
        "\n",
        "for root, _, files in os.walk(extracted_folder):\n",
        "    for file in files:\n",
        "        if file.lower().endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(root, file)\n",
        "            print(f\"Extracting text from {pdf_path}...\")\n",
        "            extracted_text = extract_text_from_pdf(pdf_path)\n",
        "            if extracted_text:\n",
        "                pdf_texts[file] = extracted_text\n",
        "                print(f\"Successfully extracted text from {file}\")\n",
        "            else:\n",
        "                print(f\"No text extracted from {file}\")\n",
        "\n",
        "# Now 'pdf_texts' is a dictionary where keys are PDF filenames and values are the extracted text.\n",
        "# The next crucial step is to process this raw text into the desired Question Answering format.\n",
        "# This part is highly dependent on the structure and content of your notes and will likely require\n",
        "# significant manual effort or more advanced NLP techniques to automate.\n",
        "print(\"\\nText extraction complete. The extracted text is stored in the 'pdf_texts' dictionary.\")\n",
        "print(\"The next step is to structure this text into Question-Answering pairs for training.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting text from uploaded_data/daa_gpt-main/DevOps-Notes.pdf...\n",
            "Successfully extracted text from DevOps-Notes.pdf\n",
            "\n",
            "Text extraction complete. The extracted text is stored in the 'pdf_texts' dictionary.\n",
            "The next step is to structure this text into Question-Answering pairs for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: now structure it for trainig\n",
        "\n",
        "from google.colab import files\n",
        "import io\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "from datasets import load_dataset, Dataset\n",
        "# Removed Streamlit imports as this code is for training, not a Streamlit app\n",
        "from transformers import pipeline\n",
        "import fitz # PyMuPDF\n",
        "import os\n",
        "import json # Import json for handling data in SQuAD format\n",
        "# Add these imports for data processing\n",
        "from tqdm.auto import tqdm\n",
        "from accelerate import Accelerator # Import Accelerator for potentially larger models\n",
        "\n",
        "# ## Task\n",
        "# Create a tutor chatbot by uploading files and training a model.\n",
        "# ## Understand the requirements\n",
        "#\n",
        "# ### Subtask:\n",
        "# Clarify the type of tutoring the chatbot should provide and the format of the uploaded files.\n",
        "#\n",
        "# **Reasoning**:\n",
        "# Engage with the user to clarify the type of tutoring, the level, and the file formats they plan to upload to create the tutor chatbot.\n",
        "#\n",
        "# # Removed conversational prints as this file focuses on the code execution flow.\n",
        "\n",
        "# ## Data preparation\n",
        "#\n",
        "# ### Subtask:\n",
        "# Load and preprocess the uploaded files to extract the relevant information for the chatbot. This might involve techniques like text extraction, cleaning, and structuring the data.\n",
        "#\n",
        "# ## Data preparation\n",
        "#\n",
        "# ### Subtask:\n",
        "# Load and preprocess the uploaded files to extract the relevant information for the chatbot. This might involve techniques like text extraction, cleaning, and structuring the data.\n",
        "#\n",
        "# **Reasoning**:\n",
        "# Request the user to specify the file types they will be uploading and explain why this information is needed to proceed with data loading and preprocessing.\n",
        "#\n",
        "# # Removed conversational prints.\n",
        "\n",
        "# ## Choose a model\n",
        "#\n",
        "# ### Subtask:\n",
        "# Choose a suitable model architecture for the chatbot.\n",
        "#\n",
        "# **Reasoning**:\n",
        "# Based on the task requirements for a tutor chatbot and the typical capabilities needed for conversational AI and question answering from potentially unstructured text data, a transformer-based architecture, specifically a pre-trained language model fine-tuned for question answering, is a suitable choice. This approach leverages the extensive knowledge embedded in large pre-trained models and can be adapted to the specific tutoring domain with relatively less data than training a model from scratch. Given that the file types are not yet known for sure but are expected to contain text-based educational content, a model adept at processing and understanding natural language is necessary.\n",
        "#\n",
        "# # Removed conversational prints.\n",
        "\n",
        "# ## Model training\n",
        "#\n",
        "# ### Subtask:\n",
        "# Train the chosen model using the prepared data.\n",
        "#\n",
        "# ## Model training\n",
        "#\n",
        "# ### Subtask:\n",
        "# Train the chosen model using the prepared data.\n",
        "#\n",
        "# **Reasoning**:\n",
        "# Acknowledge that data preparation is blocked and explain that model training cannot proceed without prepared data, and briefly reiterate the next steps once data is ready.\n",
        "#\n",
        "# # Removed conversational prints.\n",
        "\n",
        "# ## Model evaluation\n",
        "#\n",
        "# ### Subtask:\n",
        "# Evaluate the trained model's performance to ensure it meets the requirements.\n",
        "#\n",
        "# **Reasoning**:\n",
        "# Acknowledge the dependency on data preparation and model training for evaluation, state that metrics will be determined and executed after successful training, and explain the likely nature of the evaluation.\n",
        "#\n",
        "# # Removed conversational prints.\n",
        "\n",
        "# ## Deployment\n",
        "#\n",
        "# ### Subtask:\n",
        "# Deploy the trained chatbot model so it can be used to interact with users.\n",
        "#\n",
        "# **Reasoning**:\n",
        "# Acknowledge that deployment is dependent on successful data preparation, model training, and model evaluation. Explain the deployment strategy and mention that the chosen method will involve making the trained model accessible for real-time interaction. State that the specific technical steps for deployment will be provided once the model is ready.\n",
        "#\n",
        "# # Removed conversational prints.\n",
        "\n",
        "\n",
        "# Install necessary libraries if not already installed\n",
        "!pip install transformers datasets\n",
        "!pip install accelerate -U\n",
        "!pip install pymupdf # For PDF processing\n",
        "\n",
        "# Step 1: Upload files\n",
        "print(\"Please upload the files you want to use for training the tutor chatbot.\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "extracted_folder = \"uploaded_data\"\n",
        "os.makedirs(extracted_folder, exist_ok=True)\n",
        "\n",
        "for fn, content in uploaded.items():\n",
        "    file_path = os.path.join(extracted_folder, fn)\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(content)\n",
        "    print(f'User uploaded file \"{fn}\" with length {len(content)} bytes to {file_path}')\n",
        "\n",
        "    # If the uploaded file is a zip, extract it\n",
        "    if fn.endswith('.zip'):\n",
        "        print(f\"Extracting zip file: {fn}\")\n",
        "        import zipfile\n",
        "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extracted_folder)\n",
        "        print(f\"Extracted contents to {extracted_folder}\")\n",
        "        # Remove the zip file after extraction to save space\n",
        "        os.remove(file_path)\n",
        "\n",
        "\n",
        "# Step 2: Extract text from supported file types (PDF)\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with fitz.open(pdf_path) as doc:\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "pdf_texts = {}\n",
        "for root, _, files in os.walk(extracted_folder):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        if file.lower().endswith(\".pdf\"):\n",
        "            print(f\"Extracting text from {file_path}...\")\n",
        "            extracted_text = extract_text_from_pdf(file_path)\n",
        "            if extracted_text:\n",
        "                pdf_texts[file] = extracted_text\n",
        "                print(f\"Successfully extracted text from {file}\")\n",
        "            else:\n",
        "                print(f\"No text extracted from {file}\")\n",
        "        elif file.lower().endswith(('.txt', '.md', '.json')): # Add other text-based formats\n",
        "             print(f\"Reading text from {file_path}...\")\n",
        "             try:\n",
        "                 with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                     text = f.read()\n",
        "                     pdf_texts[file] = text # Using pdf_texts dict to store all text\n",
        "                     print(f\"Successfully read text from {file}\")\n",
        "             except Exception as e:\n",
        "                 print(f\"Error reading text from {file_path}: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\nText extraction complete. Extracted text is stored in memory.\")\n",
        "print(f\"Found text from {len(pdf_texts)} files.\")\n",
        "\n",
        "# Step 3: Structure data for Question Answering (SQuAD-like format)\n",
        "# This is the MOST CRITICAL and CHALLENGING step.\n",
        "# You need to convert the raw text into contexts and question-answer pairs.\n",
        "# The provided example is a placeholder and likely requires significant manual effort\n",
        "# or more advanced NLP techniques (like question generation, answer span extraction)\n",
        "# to automate based on YOUR SPECIFIC content.\n",
        "\n",
        "# For demonstration, let's create a very simple data structure.\n",
        "# A more robust solution would involve splitting text into paragraphs/chunks (contexts)\n",
        "# and then potentially manually creating or automatically generating Q&A pairs.\n",
        "\n",
        "processed_data = []\n",
        "\n",
        "# Simple approach: Treat each document as a single context\n",
        "for file_name, text in pdf_texts.items():\n",
        "    if text.strip(): # Only process if text is not empty\n",
        "        # For training, we need Q&A pairs. Since we don't have them automatically,\n",
        "        # this part requires manual creation or advanced NLP.\n",
        "        # For this example, we'll create a dummy Q&A structure.\n",
        "        # You NEED to replace this with YOUR ACTUAL data structure.\n",
        "        processed_data.append({\n",
        "            'title': file_name, # Use filename as title\n",
        "            'paragraphs': [\n",
        "                {\n",
        "                    'context': text,\n",
        "                    'qas': [\n",
        "                        # Add actual questions and answers from your data here.\n",
        "                        # Example if you manually create one Q&A pair per doc:\n",
        "                        # {\n",
        "                        #     'id': f'{file_name}_q1',\n",
        "                        #     'question': 'What is the main topic?', # Replace with a real question\n",
        "                        #     'answers': [{'answer_start': 0, 'text': text[:50]}] # Replace with a real answer span\n",
        "                        # }\n",
        "                        # Or if you have a separate file with Q&A: Load and merge it here.\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        })\n",
        "\n",
        "# --- Data Processing Example (SQuAD-like structure generation - highly dependent on your data) ---\n",
        "# This is a basic example of how you *might* convert text chunks into a format\n",
        "# suitable for Q&A training, assuming you can split your text into paragraphs\n",
        "# that serve as contexts. You will likely need to generate or manually add questions and answers.\n",
        "\n",
        "def create_qa_pairs(text, document_title, chunk_size=500, overlap=50):\n",
        "    \"\"\"\n",
        "    Basic text chunking. You'll need to add actual Q&A generation/parsing.\n",
        "    This function is a placeholder.\n",
        "    \"\"\"\n",
        "    qas = []\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap\n",
        "\n",
        "    # This is where you would add logic to create Q&A pairs for each chunk.\n",
        "    # For demonstration, we'll just use the chunks as contexts without Q&A.\n",
        "    # In a real scenario, you might use libraries for question generation\n",
        "    # or parse existing Q&A if your files contain them.\n",
        "    paragraphs = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "         # This dummy QA needs to be replaced with real data\n",
        "         dummy_qa = {\n",
        "             'id': f\"{document_title}_chunk_{i}_q0\",\n",
        "             'question': \"What is this chunk about?\", # You need real questions\n",
        "             'answers': [{'answer_start': 0, 'text': chunk[:50].strip()}] # And real answer spans\n",
        "         }\n",
        "         paragraphs.append({\n",
        "             'context': chunk,\n",
        "             'qas': [] # Start with no Q&A, you need to populate this\n",
        "         })\n",
        "\n",
        "    return paragraphs\n",
        "\n",
        "\n",
        "squad_format_data = []\n",
        "qa_id_counter = 0\n",
        "\n",
        "for file_name, text in pdf_texts.items():\n",
        "    if text.strip():\n",
        "        # Here you would process the text into paragraphs and add Q&A\n",
        "        # This example splits into paragraphs based on double newlines,\n",
        "        # but you need a method to generate or parse Q&A for these paragraphs.\n",
        "        paragraphs = text.split('\\n\\n')\n",
        "        formatted_paragraphs = []\n",
        "        for i, para in enumerate(paragraphs):\n",
        "            if para.strip():\n",
        "                 # Create dummy QA. YOU MUST REPLACE THIS.\n",
        "                 # A real implementation needs actual questions and answers with start positions.\n",
        "                 # This is just to match the SQuAD data structure.\n",
        "                 dummy_qas = []\n",
        "                 # If you had a separate file with Q&A for this document, you would load it here.\n",
        "                 # For example, if you have a list of {question: \"...\", answer: \"...\"}\n",
        "                 # for this paragraph, you would find the answer_start in the paragraph text.\n",
        "\n",
        "                 # Example of adding one dummy question per paragraph (replace with your logic)\n",
        "                 if len(para) > 20: # Only add dummy if paragraph is long enough\n",
        "                     question_text = f\"Can you summarize this part?\"\n",
        "                     answer_text = para[:min(len(para), 50)].strip() # First 50 chars as dummy answer\n",
        "                     answer_start = para.find(answer_text)\n",
        "                     if answer_start != -1:\n",
        "                          qa_id_counter += 1\n",
        "                          dummy_qas.append({\n",
        "                              'id': f\"manual_qa_{qa_id_counter}\",\n",
        "                              'question': question_text,\n",
        "                              'answers': [{'answer_start': answer_start, 'text': answer_text}]\n",
        "                          })\n",
        "\n",
        "                 if dummy_qas: # Only add if there are questions\n",
        "                    formatted_paragraphs.append({\n",
        "                        'context': para.strip(),\n",
        "                        'qas': dummy_qas\n",
        "                    })\n",
        "\n",
        "        if formatted_paragraphs:\n",
        "             squad_format_data.append({\n",
        "                 'title': file_name,\n",
        "                 'paragraphs': formatted_paragraphs\n",
        "             })\n",
        "\n",
        "# Flatten the SQuAD-like structure into a list of dictionaries for the Hugging Face Dataset\n",
        "flattened_data = []\n",
        "for article in squad_format_data:\n",
        "    for paragraph in article['paragraphs']:\n",
        "        for qa in paragraph['qas']:\n",
        "             flattened_data.append({\n",
        "                'id': qa['id'],\n",
        "                'context': paragraph['context'],\n",
        "                'question': qa['question'],\n",
        "                'answers': qa['answers'] # This should be a list of answer dicts as in SQuAD\n",
        "             })\n",
        "\n",
        "# Check if any data was successfully processed\n",
        "if not flattened_data:\n",
        "    print(\"\\nError: No valid question-answer pairs were generated or found from the uploaded files.\")\n",
        "    print(\"Please ensure your data preparation step correctly structures the text into contexts and Q&A pairs similar to the SQuAD format.\")\n",
        "    print(\"Model training cannot proceed without data in the correct format.\")\n",
        "else:\n",
        "    print(f\"\\nSuccessfully formatted {len(flattened_data)} question-answer pairs for training.\")\n",
        "\n",
        "    # Convert your processed data into a Hugging Face Dataset object\n",
        "    dataset = Dataset.from_list(flattened_data)\n",
        "\n",
        "    # Step 4: Choose a pre-trained model and tokenizer\n",
        "    model_name = \"distilbert-base-uncased\" # You can choose other models\n",
        "    print(f\"\\nLoading tokenizer and model: {model_name}\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "    print(\"Tokenizer and model loaded.\")\n",
        "\n",
        "    # Step 5: Preprocess the dataset for the model\n",
        "    # This preprocess_function is designed for SQuAD-like data\n",
        "    def preprocess_function(examples):\n",
        "        questions = [q.strip() for q in examples[\"question\"]]\n",
        "        inputs = tokenizer(\n",
        "            questions,\n",
        "            examples[\"context\"],\n",
        "            max_length=384, # Maximum input length the model can handle\n",
        "            truncation=\"only_second\", # Truncate the context if it's too long\n",
        "            return_offsets_mapping=True, # Needed to find answer start/end positions\n",
        "            padding=\"max_length\", # Pad to the max_length\n",
        "        )\n",
        "\n",
        "        offset_mapping = inputs.pop(\"offset_mapping\")\n",
        "        answers = examples[\"answers\"]\n",
        "        start_positions = []\n",
        "        end_positions = []\n",
        "\n",
        "        for i, offset in enumerate(offset_mapping):\n",
        "            start_char = answers[i][0][\"answer_start\"]\n",
        "            end_char = start_char + len(answers[i][0][\"text\"])\n",
        "\n",
        "            sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "            # Find the start and end of the context\n",
        "            idx = 0\n",
        "            while sequence_ids[idx] != 1: # Find the first token of the context\n",
        "                idx += 1\n",
        "            context_start_token_idx = idx\n",
        "            while idx < len(sequence_ids) and sequence_ids[idx] == 1: # Find the last token of the context\n",
        "                idx += 1\n",
        "            context_end_token_idx = idx - 1\n",
        "\n",
        "            # If the answer is not fully inside the context, label it (0, 0)\n",
        "            # Note: This can happen if truncation removed part of the answer\n",
        "            # Or if the provided answer_start is incorrect\n",
        "            if offset[context_start_token_idx][0] > start_char or offset[context_end_token_idx][1] < end_char:\n",
        "                start_positions.append(0)\n",
        "                end_positions.append(0)\n",
        "            else:\n",
        "                # Otherwise it's the start and end token positions of the answer\n",
        "                # Find the first token of the answer by iterating through the context tokens\n",
        "                idx = context_start_token_idx\n",
        "                while idx <= context_end_token_idx and offset[idx][0] <= start_char:\n",
        "                    idx += 1\n",
        "                start_positions.append(idx - 1) # The previous token was the start\n",
        "\n",
        "                # Find the last token of the answer by iterating backward\n",
        "                idx = context_end_token_idx\n",
        "                while idx >= context_start_token_idx and offset[idx][1] >= end_char:\n",
        "                    idx -= 1\n",
        "                end_positions.append(idx + 1) # The next token is the end (exclusive)\n",
        "\n",
        "        inputs[\"start_positions\"] = start_positions\n",
        "        inputs[\"end_positions\"] = end_positions\n",
        "        return inputs\n",
        "\n",
        "    # Apply preprocessing to the dataset\n",
        "    print(\"\\nTokenizing and preprocessing dataset...\")\n",
        "    # Using batched=True for efficiency\n",
        "    tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
        "    print(\"Dataset preprocessing complete.\")\n",
        "    print(tokenized_dataset)\n",
        "\n",
        "\n",
        "    # Step 6: Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\", # Directory to save results and checkpoints\n",
        "        # evaluation_strategy=\"epoch\", # Evaluate after each epoch (needs eval_dataset)\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8, # Reduce if hitting memory limits\n",
        "        # per_device_eval_batch_size=8,\n",
        "        num_train_epochs=3, # Number of training epochs\n",
        "        weight_decay=0.01,\n",
        "        save_steps=10_000, # Save checkpoint every 10000 steps\n",
        "        save_total_limit=2, # Keep only the latest 2 checkpoints\n",
        "        fp16=True if torch.cuda.is_available() else False, # Enable mixed precision training if GPU available\n",
        "        report_to=\"none\" # Disable reporting to external services like W&B unless configured\n",
        "    )\n",
        "    print(\"\\nTraining arguments defined.\")\n",
        "\n",
        "    # Step 7: Create the Trainer\n",
        "    # You should ideally split your data into train and evaluation sets.\n",
        "    # For simplicity, we'll use the whole dataset for training here,\n",
        "    # but in a real project, split before creating the Dataset object.\n",
        "    # For example: dataset = dataset.train_test_split(test_size=0.1)\n",
        "    # train_dataset = tokenized_dataset['train']\n",
        "    # eval_dataset = tokenized_dataset['test']\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        # eval_dataset=eval_dataset, # Uncomment if you have an evaluation split\n",
        "        tokenizer=tokenizer, # Pass tokenizer to the Trainer\n",
        "    )\n",
        "    print(\"Trainer initialized.\")\n",
        "\n",
        "    # Step 8: Train the model\n",
        "    print(\"\\nStarting model training...\")\n",
        "    trainer.train()\n",
        "    print(\"Model training complete.\")\n",
        "\n",
        "    # Step 9: Save the trained model and tokenizer\n",
        "    model_save_path = \"./trained_qa_model\"\n",
        "    os.makedirs(model_save_path, exist_ok=True)\n",
        "    trainer.save_model(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    print(f\"Trained model saved to {model_save_path}\")\n",
        "\n",
        "    # Clean up extracted files if needed\n",
        "    # !rm -rf {extracted_folder}\n",
        "    # print(f\"Cleaned up extracted files in {extracted_folder}\")\n",
        "\n",
        "\n",
        "    # You are now ready to move to the evaluation and deployment steps.\n",
        "    # The Streamlit and pipeline code from the original extract is for deployment/inference\n",
        "    # and should be in a separate script/notebook if you want to run inference.\n",
        "    # For evaluation, you would use trainer.evaluate() on an eval_dataset.\n",
        "    # For deployment, you would load the saved model and tokenizer and use a pipeline.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a53e8eb0cf51414fa7a9ad4abbd65e6d",
            "c4df821ab14d4144bc31559aebf95e2b",
            "ec04bcc252b541919343917bc9907009",
            "35381c867ed5474db2418e031396601e",
            "4dc1878967d044f48aaa6a3db3e73ba7",
            "28512038a4d449edabdff3ed686faea5",
            "214ccb8db89246c7aa0118d4c6cdc0df",
            "a63a4cdd07234841b6fba3a4e9d84283",
            "f0347bca12f84efb890ad960948cf5c4",
            "8fcd098c7e0944ed8b7ecead526e99dd",
            "042a23864a9e44cab18e617ee2597f39",
            "9b6e89b6e1d04019a19420dc1b7d3d81",
            "a9c734c579814360a1b0b74e994ef581",
            "8d5daf54334d4a72838fbbdd764dd355",
            "bfa5ffc7d7bb4dc79b9e376a367a9261",
            "a421cf45221d41b8855650a748bd68e4",
            "c8a8cd80596147bbaf995673251f1aa4",
            "9733d86304184e0b836721cb93e6f259",
            "95c0e0d8ead34ba697c1b3be8ed573f0",
            "1d2c85ada5c9403aae8441f453c082fb",
            "934a0ad0f0a347a290914e04c7a83df1",
            "0c220dfcceeb49e3a4b481c1d369d304",
            "1986a563b6fb46f78f96f018164ab22b",
            "02b09114801b49d6bde413cf9ea4a417",
            "de8bcc1ddab74a7b86ef104c3f752b55",
            "4651a6a505b44822b180d0c3c3a708d6",
            "465a694a7f8546c88eb673df1ffac171",
            "1fe01b050981435aaa69a9f4b395e23d",
            "84e2aeb72d36413cb1fbcf3634446a77",
            "add85205bd1a44a3981696db172c89cf",
            "92d3c282457d40588a742f6fffac2952",
            "74eaed45025640edaf19ce3dfbccc4cb",
            "975972af84a649bfa82de33036770525",
            "fdd5a1487c8940148601c07a5ab17502",
            "ec29b165364f4b38a4d6b062e81adda0",
            "32ab07f9346c401ba9ce623a3345cd20",
            "d1938e1acc6c4ba1a133ba4f1084f7df",
            "9503b10dff414959a144cf4ecea37518",
            "82dc2f7cd6564f50a67c9597377bfe0f",
            "b9b80ffc4cde4323b7cf5da2e729bb39",
            "48e476d91c034a71b1534ae46fdeabd4",
            "dc673d0ab9ba41af89171f2d0807e580",
            "b27385a37b524df5b6a1db832ed1e0a4",
            "523a9ba7cdeb4ca0af485fdbbb6d0495",
            "f10fba74bfd64d919923c4bd0581d495",
            "5785c973cc464f309938a72d8742cf76",
            "7139b403b4a2426fb07489859019cebc",
            "f7772f603f3b4648b963a625e5b66542",
            "770b6f17358b441eb12cf88e3ed47604",
            "d1dea71beaaa4c929808863414363e52",
            "b95a15bd195047e8a73dc7f44e6f53b2",
            "992695db437040de94b906f8aa8228bc",
            "26211c69f49f4a75a6915d844ad97ef9",
            "3f8feac08a494b28a7cbc0a51c86b4d1",
            "52192371a3144c5d8ff768889d4851f4",
            "84098f5f42fb418ba1f879bc713fa960",
            "87624f56c0424f43b1d28b7dcdc9697e",
            "96a0989d1e594b43a373757e44771049",
            "1d6b8def3ece4b9e972265b73f7813ad",
            "d86c22c037ed4c69af1373da32029e18",
            "816a405cdb68465fbba88e66c26ec57f",
            "4fe9882cd1d54865a7192b988af1c5a7",
            "4d21606ab9904151a96dcc5ed6256ed3",
            "1eedc92933dd45ebb8cbd2cd1cff643f",
            "227781ce1b1f4b78801390c1e35af64a",
            "b9fced6686b2495690cb867c005ed3c0"
          ]
        },
        "id": "S6JMX_ErLU90",
        "outputId": "aa8b2261-8548-4f23-b9db-50bc28421761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.33.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.6.15)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.26.1)\n",
            "Please upload the files you want to use for training the tutor chatbot.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bba5a1a2-c585-4db3-9993-c4a802b5e403\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bba5a1a2-c585-4db3-9993-c4a802b5e403\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving DevOps-Notes.pdf to DevOps-Notes.pdf\n",
            "User uploaded file \"DevOps-Notes.pdf\" with length 3131128 bytes to uploaded_data/DevOps-Notes.pdf\n",
            "Extracting text from uploaded_data/DevOps-Notes.pdf...\n",
            "Successfully extracted text from DevOps-Notes.pdf\n",
            "Extracting text from uploaded_data/daa_gpt-main/DevOps-Notes.pdf...\n",
            "Successfully extracted text from DevOps-Notes.pdf\n",
            "Reading text from uploaded_data/daa_gpt-main/requirements.txt...\n",
            "Successfully read text from requirements.txt\n",
            "Reading text from uploaded_data/daa_gpt-main/runtime.txt...\n",
            "Successfully read text from runtime.txt\n",
            "Reading text from uploaded_data/daa_gpt-main/README.md...\n",
            "Successfully read text from README.md\n",
            "Reading text from uploaded_data/daa_gpt-main/Notes.txt...\n",
            "Error reading text from uploaded_data/daa_gpt-main/Notes.txt: 'utf-8' codec can't decode byte 0xb7 in position 29643: invalid start byte\n",
            "Reading text from uploaded_data/daa_gpt-main/notes.txt...\n",
            "Successfully read text from notes.txt\n",
            "Reading text from uploaded_data/daa_gpt-main/.devcontainer/devcontainer.json...\n",
            "Successfully read text from devcontainer.json\n",
            "\n",
            "Text extraction complete. Extracted text is stored in memory.\n",
            "Found text from 6 files.\n",
            "\n",
            "Successfully formatted 181 question-answer pairs for training.\n",
            "\n",
            "Loading tokenizer and model: distilbert-base-uncased\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a53e8eb0cf51414fa7a9ad4abbd65e6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b6e89b6e1d04019a19420dc1b7d3d81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1986a563b6fb46f78f96f018164ab22b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdd5a1487c8940148601c07a5ab17502"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f10fba74bfd64d919923c4bd0581d495"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer and model loaded.\n",
            "\n",
            "Tokenizing and preprocessing dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/181 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84098f5f42fb418ba1f879bc713fa960"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset preprocessing complete.\n",
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
            "    num_rows: 181\n",
            "})\n",
            "\n",
            "Training arguments defined.\n",
            "Trainer initialized.\n",
            "\n",
            "Starting model training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1-2290913342.py:399: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='69' max='69' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [69/69 21:21, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete.\n",
            "Trained model saved to ./trained_qa_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: test the model\n",
        "\n",
        "# Step 10: Test the trained model\n",
        "print(\"\\nTesting the trained model...\")\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model_path = \"./trained_qa_model\"\n",
        "try:\n",
        "    loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    loaded_model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "    print(\"Trained model and tokenizer loaded successfully.\")\n",
        "\n",
        "    # Create a question-answering pipeline\n",
        "    # Set device to 0 for GPU if available, -1 for CPU\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=loaded_model, tokenizer=loaded_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
        "    print(\"Question-answering pipeline created.\")\n",
        "\n",
        "    # Example of how to use the pipeline\n",
        "    # You need a context (from your original data) and a question.\n",
        "    # Let's try to use one of the contexts from the loaded data if available.\n",
        "    if flattened_data:\n",
        "        # Use the context and question from the first example in your processed data\n",
        "        example_qa = flattened_data[0]\n",
        "        context_for_test = example_qa['context']\n",
        "        question_for_test = example_qa['question'] # Use the question used for training\n",
        "        # You can also formulate a new question based on the context\n",
        "\n",
        "        print(f\"\\nContext used for testing:\\n{context_for_test[:500]}...\") # Print first 500 chars\n",
        "        print(f\"\\nQuestion for testing: {question_for_test}\")\n",
        "\n",
        "        # Get the answer from the pipeline\n",
        "        try:\n",
        "            answer = qa_pipeline(question=question_for_test, context=context_for_test)\n",
        "            print(\"\\nModel Prediction:\")\n",
        "            print(f\"Answer: {answer['answer']}\")\n",
        "            print(f\"Score: {answer['score']:.4f}\")\n",
        "            print(f\"Start: {answer['start']}\")\n",
        "            print(f\"End: {answer['end']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during pipeline inference: {e}\")\n",
        "            print(\"Please ensure the context and question are in the correct format.\")\n",
        "\n",
        "        # You can also try with a different question or a different context\n",
        "        # For example:\n",
        "        # new_question = \"What is mentioned after photosynthesis?\"\n",
        "        # if len(flattened_data) > 0: # Use the same context if available\n",
        "        #     context_for_new_question = flattened_data[0]['context']\n",
        "        #     print(f\"\\nTesting with a new question: {new_question}\")\n",
        "        #     try:\n",
        "        #          answer_new = qa_pipeline(question=new_question, context=context_for_new_question)\n",
        "        #          print(\"\\nModel Prediction for new question:\")\n",
        "        #          print(f\"Answer: {answer_new['answer']}\")\n",
        "        #          print(f\"Score: {answer_new['score']:.4f}\")\n",
        "        #     except Exception as e:\n",
        "        #          print(f\"Error during pipeline inference with new question: {e}\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"No processed data available to use as context for testing.\")\n",
        "        print(\"Please ensure your data processing step successfully generated 'flattened_data'.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading the trained model or tokenizer: {e}\")\n",
        "    print(\"Please ensure the model was trained and saved successfully to the './trained_qa_model' directory.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1RHagFbS5wU",
        "outputId": "3fc82494-80bc-417f-90da-ba3804e0f5c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing the trained model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained model and tokenizer loaded successfully.\n",
            "Question-answering pipeline created.\n",
            "\n",
            "Context used for testing:\n",
            "DEVOPS\n",
            "GIT:-\n",
            "Git is a distributed version-control system for tracking changes in source \n",
            "code during software development. It is designed for coordinating work \n",
            "among programmers, but it can be used to track changes in any set of files. Its \n",
            "goals include speed, data integrity, and support for distributed, non-linear \n",
            "workflows. \n",
            "Git was created by Linus Torvalds in 2005 for development of the Linux kernel, \n",
            "with other kernel developers contributing to its initial development.Its current \n",
            "mainta...\n",
            "\n",
            "Question for testing: Can you summarize this part?\n",
            "\n",
            "Model Prediction:\n",
            "Answer: module 'ssl'\n",
            "apache2_default\n",
            "Score: 0.0068\n",
            "Start: 11739\n",
            "End: 11767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NW2jvsPVTpuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: how to keep this model and files in the github\n",
        "\n",
        "# To keep this model and files in a GitHub repository, you need to:\n",
        "\n",
        "# 1. Connect your Google Colab notebook to GitHub.\n",
        "#    - You can do this by going to File -> Save a copy in GitHub.\n",
        "#    - This will create a new repository or save to an existing one.\n",
        "\n",
        "# 2. Ensure your .ipynb file (the notebook itself) is committed and pushed to the repository.\n",
        "\n",
        "# 3. Save your trained model files.\n",
        "#    - The code already saves the model and tokenizer to './trained_qa_model'.\n",
        "#    - You need to ensure these saved files are also committed to your GitHub repository.\n",
        "#    - Since these are large files, consider using Git Large File Storage (LFS).\n",
        "\n",
        "# 4. Add the necessary files and directories to Git.\n",
        "#    - This includes the notebook file (.ipynb) and the 'trained_qa_model' directory with its contents.\n",
        "\n",
        "# 5. Commit your changes.\n",
        "\n",
        "# 6. Push your changes to your GitHub repository.\n",
        "\n",
        "# --- Git Commands within Colab ---\n",
        "# After running the training and saving the model, you can use Git commands\n",
        "# directly in Colab cells by prefixing them with '!'.\n",
        "\n",
        "# If you haven't already cloned the repo:\n",
        "# !git clone <your_repo_url>\n",
        "# %cd <your_repo_directory>\n",
        "\n",
        "# Navigate to the directory where your notebook is saved if you saved it manually\n",
        "# to a cloned repo. If you used \"Save a copy in GitHub\", the notebook is likely\n",
        "# at the root of the cloned repo directory.\n",
        "\n",
        "# Add the model directory and notebook to staging area\n",
        "# !git add .\n",
        "# Alternatively, specifically add the model directory:\n",
        "# !git add trained_qa_model/\n",
        "# !git add <your_notebook_name>.ipynb\n",
        "\n",
        "# Configure Git (replace with your details)\n",
        "# !git config --global user.email \"your_email@example.com\"\n",
        "# !git config --global user.name \"Your Name\"\n",
        "\n",
        "# Commit your changes\n",
        "# !git commit -m \"Add trained QA model and notebook\"\n",
        "\n",
        "# Push your changes to GitHub\n",
        "# You might need to configure credentials if not already set up.\n",
        "# Colab provides credential helpers.\n",
        "# !git push origin main # Or 'master' depending on your branch name\n",
        "\n",
        "# --- Important Considerations for large models ---\n",
        "# Trained models (especially language models) can be very large (hundreds of MBs to GBs).\n",
        "# Standard Git is not designed for large binary files.\n",
        "# Using Git LFS (Large File Storage) is highly recommended for storing model weights.\n",
        "\n",
        "# Steps to use Git LFS:\n",
        "# 1. Install Git LFS (usually done locally on your machine, not necessarily in Colab).\n",
        "#    !git lfs install # You can try this in Colab, but it's better on your local machine.\n",
        "# 2. Track large files (e.g., model weights) with Git LFS.\n",
        "#    !git lfs track \"trained_qa_model/*\"\n",
        "# 3. Add the .gitattributes file (created by git lfs track) to Git.\n",
        "#    !git add .gitattributes\n",
        "# 4. Add the model files as usual.\n",
        "#    !git add trained_qa_model/\n",
        "# 5. Commit and push. Git LFS will handle the large files.\n",
        "\n",
        "# Because setting up Git LFS within a transient Colab environment can be tricky\n",
        "# for first-time setup, the most common workflow is:\n",
        "# 1. Run your Colab notebook to train the model and save it locally in the Colab environment.\n",
        "# 2. Download the saved model directory ('./trained_qa_model') to your local machine.\n",
        "# 3. On your local machine, clone your GitHub repository.\n",
        "# 4. Install Git LFS locally (`git lfs install`).\n",
        "# 5. Navigate to your local repository directory.\n",
        "# 6. Track the model file types/directories with Git LFS (e.g., `git lfs track \"trained_qa_model/*\"`).\n",
        "# 7. Copy the downloaded 'trained_qa_model' directory into your local repository.\n",
        "# 8. Add, commit, and push from your local machine (`git add .`, `git commit -m \"...\"`, `git push`).\n",
        "\n",
        "# If you strictly want to do it from Colab and the model isn't excessively large\n",
        "# (under ~100MB), you might get away without LFS, but it's not recommended for models.\n",
        "# For this example, assuming `distilbert-base-uncased` fine-tuned might be manageable\n",
        "# without LFS for a small number of epochs, you can try the basic git commands.\n",
        "\n",
        "# After the model training and saving:\n",
        "# Ensure you are in the correct directory (where the 'trained_qa_model' folder is)\n",
        "# If you used \"Save a copy in GitHub\", you are likely already there.\n",
        "# !pwd # Check your current directory\n",
        "# !ls # List files to see if trained_qa_model is there\n",
        "\n",
        "# !git add trained_qa_model/\n",
        "# !git add <your_notebook_name>.ipynb # Replace with the actual name of your notebook file\n",
        "# !git commit -m \"Add trained model after fine-tuning\"\n",
        "# !git push origin main # Or 'master', adjust branch name as needed\n",
        "\n",
        "# If `git push` requires credentials and you haven't configured them,\n",
        "# Colab might pop up a dialog or you might need to use a personal access token.\n",
        "# A common way is using the credential helper:\n",
        "# !git push https://<YOUR_GITHUB_USERNAME>:<YOUR_PERSONAL_ACCESS_TOKEN>@github.com/<YOUR_GITHUB_USERNAME>/<YOUR_REPO_NAME>.git main\n",
        "# Replace placeholders with your actual GitHub username, token, and repo details.\n",
        "# Generate a personal access token in your GitHub settings (Developer settings -> Personal access tokens -> Tokens (classic)). Give it 'repo' scope.\n",
        "\n",
        "\n",
        "# --- Summary of Colab Steps to Push Model to GitHub (Basic, without LFS setup in Colab) ---\n",
        "# 1. Run notebook up to model saving.\n",
        "# 2. Make sure you are in the root directory of your cloned repo.\n",
        "# 3. !git add trained_qa_model/\n",
        "# 4. !git add <your_notebook_name>.ipynb\n",
        "# 5. !git commit -m \"Commit trained model and notebook\"\n",
        "# 6. !git push origin main # Or your branch\n",
        "\n",
        "# --- Alternative: Download and Push Locally ---\n",
        "# This is often easier for large models.\n",
        "# After training and saving:\n",
        "# 1. Zip the model directory: !zip -r trained_qa_model.zip trained_qa_model/\n",
        "# 2. Download the zip: files.download('trained_qa_model.zip')\n",
        "# 3. On your local machine:\n",
        "#    - git clone <your_repo_url>\n",
        "#    - cd <your_repo_name>\n",
        "#    - (Install Git LFS locally if needed and track files)\n",
        "#    - Unzip the downloaded file: unzip trained_qa_model.zip\n",
        "#    - git add trained_qa_model/\n",
        "#    - git add <your_notebook_name>.ipynb\n",
        "#    - git commit -m \"Add trained model\"\n",
        "#    - git push origin main\n",
        "\n",
        "# The provided Python code snippet ends with testing the loaded model, which is fine.\n",
        "# The part relevant to *keeping the model and files in GitHub* involves using Git\n",
        "# commands after the model training and saving steps are complete.\n"
      ],
      "metadata": {
        "id": "wCgWNpWgTsV-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
